{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/dmlls/jizt/c2d7b9b81783e298d1898b5743b147d1faff8f29/images/JIZT-logo.svg\" title=\"JIZT\" alt=\"JIZT\" width=\"230\" align=\"left\" style=\"margin-top:15px;margin-right:30px;\" />\n",
    "\n",
    "---\n",
    "\n",
    "### Pre-procesamiento básico del texto\n",
    "[Diego Miguel Lozano](https://github.com/dmlls) \\\n",
    "GPL-3.0 License\n",
    "\n",
    "*Última actualización: 9 de noviembre de 2020*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "Este notebook se centra en el proceso de pre-procesado del texto de entrada para adaptarlo a los modelos `BART`y `T5`, con los cuales se llevará a cabo el resumen de los mismos. Los textos empleados como ejemplo están en inglés, dado que estos modelos están optimizados para este idioma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requerimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder ejecutar este notebook, se debe tener instalada la última versión de los siguientes paquetes:\n",
    "\n",
    "- `NLTK`\n",
    "- `SpaCy` con `en_core_web_sm`\n",
    "- [`blingfire`](https://github.com/microsoft/BlingFire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-procesamiento del texto\n",
    "\n",
    "El pre-procesamiento del texto para adaptarlo a estos dos modelos va a consistir en:\n",
    "- Eliminar saltos de carro, tabuladores (`\\n`, `\\t`) y espacios sobrantes entre palabras (p. ej. `I    am` → `I am`).\n",
    "- Añadir un espacio al inicio de las frases intermedias (p. ej.: `How's it going?Great!` → `How's it going? Great!`. Esto es especialmente relevante en el caso del modelo `BART`, que tiene en cuenta ese espacio inicial para distinguir entre frases iniciales y frases intermedias.\n",
    "- Establecer un mecanismo que permita dividir el texto en frases. Esto es importante dado que los modelos tienen un tamaño de entrada máximo (que viene dado en número de tókenes codificados). Tener el texto dividio en frases nos permite ajustar el tamaño del texto de entrada manteniendo la coeherencia del texto, esto es, sin partir frases, con lo cual perderíamos el sentido de las mismas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, se va a asumir que:\n",
    "- El punto (`.`) indica el final de una frase solo si la siguiente palabra empieza con una *letra* mayúscula. Por ejemplo: `Your idea is interesting. However, I would... ` se separaría en dos frases. Sin embargo: `We already mentioned in section 1.1 that this example shows...` conformaría una única frase. Lo mismo ocurre en el caso de los signos de interrogación (`?`) y de exclamación (`!`). Por ejemplo: `She asked \"How's it going?\", and I said \"Great!\".` se tomará como una sola frase.\n",
    "\n",
    "\n",
    "- Además, con la restricción de que para conformar una nueva frase, el siguiente carácter tras el punto, interrogación o exclamación sea una *letra*, se clasifican correctamente signos de puntuación como los puntos suspensivos. No obstante, esta suposición fallaría en situaciones como: `NLP (i.e. Natural Language Processing) is a subfield of Linguistics, Computer Science, and Artificial Intelligence.` en la que la división sería: `NLP (i.e.` y `Natural Language Processing) is a subfield...` es decir, dos frases, cuando en realidad solo hay una."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos un ejemplo. Emplearemos el siguiente texto (mal formateado a propósito):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How's your        day going???!It's     going...\n",
      " Let's just say it's not going to \t bad.\n"
     ]
    }
   ],
   "source": [
    "text = \"How's your        day going???!It's     going...\\n Let's just say it's not going to \\t bad.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso es eliminar los saltos de carro, tabuladores (`\\n`, `\\t`) y espacios sobrantes entre palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How's your day going???!It's going... Let's just say it's not going to bad.\n"
     ]
    }
   ],
   "source": [
    "text = ' '.join(text.split())\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, separamos el texto en frases con ayuda de la siguiente expresión regular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[^.!?]+[.!?]+[^A-Z]*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La expresión regular se divide en tres partes:\n",
    "- `[^.!?]+`: coge cualquier carácter que no sea el carácter de terminación de frase (esto es, punto, exclamación o interrogación).\n",
    "- `[.!?]+`: hasta llegar a uno o varios caracteres de terminación. El uno o varios asegura que parseamos correctamente los puntos suspensivos (`...`), exclamación-interrogación ( `!?`), o repetición de cualquiera de estos caracteres (`????`).\n",
    "- `[^A-Z]*`: además, si lo que sigue no es una letra mayúscula, cógelo también. De esta forma, no dividimos incorrectamente frases como: `As we can see in Figure 1.1, the model is overfitting`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"How's your day going???!\",\n",
       " \"It's going... \",\n",
       " \"Let's just say it's not going to bad.\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La razón por la que no usamos el método `sent_tokenize` de la librería `nltk` se debe a que este método falla en algunas ocasiones concretas. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello.Goodbye.']\n",
      "['Seriously??', \"!That can't be true.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "print(sent_tokenize(\"Hello.Goodbye.\")) # fails: there are two sentences, it parses everything as one\n",
    "print(sent_tokenize(\"Seriously??!That can't be true.\")) # fails: takes '!' as part of the second sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mientras que nuestra sencilla expresión regular las separa correctamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello.', 'Goodbye.']\n",
      "['Seriously??!', \"That can't be true.\"]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"Hello.Goodbye.\"))\n",
    "print(tokenizer.tokenize(\"Seriously??!That can't be true.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, nuestra expresión regular no es perfecta. Hay casos en los que irremediablemente falla. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I was born in 02.28.1980 in ', 'New York.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"I was born in 02.28.1980 in New York.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontrar una expresión regular que contemple este caso y los anteriores no es tarea sencilla, y en cualquier caso complicaría mucho la expresión. Por ello, ello es más fácil revisar si alguna frase ha quedado sin terminar en punto, exclamación o interrogación, y en ese caso concatenarla con la siguiente frase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I was born in 02.28.1980 in New York.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = tokenizer.tokenize(\"I was born in 02.28.1980 in New York.\")\n",
    "\n",
    "final_sentences = [sentences[0].strip()] # remove leading and trailing whitespaces\n",
    "    \n",
    "for sent in sentences[1:]:\n",
    "    sent = sent.strip()\n",
    "    # check first that sentences is not empty\n",
    "    # if the previous sentence doesn't end with a '.', '!' or '?' we concatenate the current sentence to it\n",
    "    if final_sentences[-1][-1] != '.' and final_sentences[-1][-1] != '!' and final_sentences[-1][-1] != '?':\n",
    "        final_sentences[-1] += (' ' + sent)\n",
    "    else:\n",
    "        final_sentences.append(sent)\n",
    "            \n",
    "final_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya solo nos queda juntar todos los pasos en una función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def preprocess_text(text, tokenizer=None, return_as_list=False):\n",
    "    if tokenizer is None:\n",
    "        # if next letter after period is lowercase, consider it part of the same sentence\n",
    "        # ex: \"As we can see in Figure 1.1. the sentence will not be split.\"\n",
    "        tokenizer = RegexpTokenizer(r'[^.!?]+[.!?]+[^A-Z]*')\n",
    "        # if there's no final period, add it (this makes the assumption that the last\n",
    "        # sentence is not interrogative or exclamative, i.e., ends with '?' or '!')\n",
    "        if text[-1] != '.' and text[-1] != '?' and text[-1] != '!':\n",
    "            text += '.'\n",
    "    \n",
    "    text = ' '.join(text.split()) # remove '\\n', '\\t', etc.\n",
    "    \n",
    "    sentences = tokenizer.tokenize(text)\n",
    "\n",
    "    final_sentences = [sentences[0].strip()] # remove leading and trailing whitespaces\n",
    "    \n",
    "    for sent in sentences[1:]:\n",
    "        sent = sent.strip()\n",
    "        # if the previous sentence doesn't end with a '.', '!' or '?' we concatenate the current sentence to it\n",
    "        if final_sentences[-1][-1] != '.' and final_sentences[-1][-1] != '!' and final_sentences[-1][-1] != '?':\n",
    "            final_sentences[-1] += (' ' + sent)\n",
    "        else:\n",
    "            final_sentences.append(sent)\n",
    "                                       \n",
    "    return final_sentences if return_as_list else ' '.join(final_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Hemos cubierto todos los casos? La respuesta es no. Nuestra función no tiene en cuenta las Entidades Nombradas y fallará en casos como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Elster looked worried.']\n",
      "['London is the capital of U.', 'K.']\n",
      "['The soldier was declared A.', 'W.', 'O.', 'L.']\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_text(\"Mr. Elster looked worried.\", return_as_list=True)) # fails: it's only one sentence, not two\n",
    "print(preprocess_text(\"London is the capital of U.K.\", return_as_list=True)) # fails: splits U.K.\n",
    "print(preprocess_text(\"The soldier was declared A.W.O.L.\", return_as_list=True)) # fails: splits A.W.O.L."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es por aspectos como este que los tokenizadores basados en reglas están empezando a ser reemplazados por modelos probabilísticos, los cuales ofrecen una mayor potencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos, pues, con un modelo más potente. Para ello, vamos a hacer uso de la librería `Spacy`, muy conocida junto a `NLTK` en el mundo del Procesamiento de Lenguaje Natural en Python. Usaremos además el modelo para inglés llamado `en_core_web_sm`. Este modelo implementa una red neuronal convolucional entrenada sobre `OntoNotes`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qué tal se comporta este modelo sobre los ejemplos vistos anteriormente. Para empezar, vamos a probar qué tal despempeña la tarea de Reconocimiento de Entidades Reconocidas (NER, por sus siglas en inglés):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr. Elster looked worried.',\n",
       " 'London is the capital of U.K.',\n",
       " 'The soldier was declared A.W.O.L.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_NER = [\"Mr. Elster looked worried.\", \"London is the capital of U.K.\", \"The soldier was declared A.W.O.L.\"]\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for text in texts_NER:\n",
    "    sentences += [str(sen) for sen in nlp(text).sents]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el modelo reconoce las Entidades Nombradas de manera correcta y no divide las anteriores frases erróneamente, como pasaba con nuestra función."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero, de nuevo, hay casos en los que el modelo no funciona como debería. Algunos de los ejemplos que veíamos anteriormente y que nuestra función separaba correctamente, fallan con el modelo de `Spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Seriously??!That can't be true.\",\n",
       " 'As we can see in Figure 1.1.',\n",
       " 'the model will fail.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_fail = [\"Seriously??!That can't be true.\", # fails: there are two sentences; the model sees only one\n",
    "              \"As we can see in Figure 1.1. the model will fail.\"] # fails: there's only one sentence, not two\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for text in texts_fail:\n",
    "    sentences += [str(sen) for sen in nlp(text).sents]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con todo lo visto hasta ahora, podemos hacernos la siguiente pregunta: ¿Y por qué no emplear ambos, nuestra función y el modelo preentrenado para realizar el pre-procesado del texto de la forma más precisa posible? Probemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def preprocess_text(text, tokenizer=None, return_as_list=False):\n",
    "    if tokenizer is None:\n",
    "        # if next letter after period is lowercase, consider it part of the same sentence\n",
    "        # ex: \"As we can see in Figure 1.1. the sentence will not be split.\"\n",
    "        tokenizer = RegexpTokenizer(r'[^.!?]+[.!?]+[^A-Z]*')\n",
    "        # if there's no final period, add it (this makes the assumption that the last\n",
    "        # sentence is not interrogative or exclamative, i.e., ends with '?' or '!')\n",
    "        if text[-1] != '.' and text[-1] != '?' and text[-1] != '!':\n",
    "            text += '.'\n",
    "    \n",
    "    text = ' '.join(text.split()) # remove '\\n', '\\t', etc.\n",
    "    \n",
    "    sentences = ' '.join(tokenizer.tokenize(text)).replace('  ', ' ') # ensure there's 1 whitespace at most\n",
    "    \n",
    "    sentences = [str(sent).strip() for sent in nlp(sentences).sents] # Spacy model\n",
    "\n",
    "    final_sentences = [sentences[0]]\n",
    "    \n",
    "    for sent in sentences[1:]:\n",
    "        # if the previous sentence doesn't end with a '.', '!' or '?' we concatenate the current sentence to it\n",
    "        if final_sentences[-1][-1] != '.' and final_sentences[-1][-1] != '!' and final_sentences[-1][-1] != '?':\n",
    "            final_sentences[-1] += ' ' + sent\n",
    "        # if the next sentence doesn't start with a letter or a number, concatenate it to the previous\n",
    "        elif not sent[0].isalpha() and not sent[0].isdigit():\n",
    "            final_sentences[-1] += sent\n",
    "        else:\n",
    "            final_sentences.append(sent)\n",
    "                                       \n",
    "    return final_sentences if return_as_list else ' '.join(final_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos, por última vez, los ejemplos vistos hasta ahora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"How's your day going???!\", \"It's going... Let's just say it's not going to bad.\"]\n",
      "['Hello.', 'Goodbye.']\n",
      "['Seriously??!', \"That can't be true.\"]\n",
      "['Mr. Elster looked worried.']\n",
      "['London is the capital of U. K.']\n",
      "['I was born in 02.28.1980 in New York.']\n",
      "['She asked \"How\\'s it going?\", and I said \"Great!\".']\n",
      "['As we can see in Figure 1.1. the model will fail.']\n"
     ]
    }
   ],
   "source": [
    "examples = [\"How's your        day going???!It's     going...\\n Let's just say it's not going to \\t bad.\",\n",
    "            \"Hello.Goodbye.\",\n",
    "            \"Seriously??!That can't be true.\",\n",
    "            \"Mr. Elster looked worried.\",\n",
    "            \"London is the capital of U.K.\",\n",
    "            \"I was born in 02.28.1980 in New York\",\n",
    "            \"She asked \\\"How's it going?\\\", and I said \\\"Great!\\\"\",\n",
    "            \"As we can see in Figure 1.1. the model will fail.\"]\n",
    "\n",
    "for text in examples:\n",
    "    print(preprocess_text(text, return_as_list=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora, es el mejor resultado obtenido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, en situaciones muy concretas, como en el caso de la última frase, el modelo sigue fallando. Además, es una función bastante costosa en cuanto a tiempo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 ms ± 2.78 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "quite_long_text = \"\"\"Our country is being held hostage by mad scientists and MPs afraid of blame.And the most important vision they should be using, hindsight they all appear to be blind too. I will try and break it down in this long thread. #WhyAreTheyDoingThis #pensionerprisoners #Covid_19 1 - Covid-19 arrived far earlier than anyone knew. It was likely in Britain in November or December and by January and February it was spreading rapidly. This took our government and other countries by surprise. And the resulting overload on health services was massive. 2 - there was little choice by mid March for most leaders and action to prevent a catastrophic failure and collapse of the NHS was essential. Cases were flooding in stadd sickness in NHS was rising fast. We were on the brink. 3 - the virus was so new the medical knowledge just hadn\\'t accumulated to make good decisions on treatments. Doctors were fighting for peoples lives based on at times best guess using all their previous knowledge of other viruses. But covid was different and its impact nuanced. 4 - Although lockdown was a hammer to hit a flea it seemed at the time the only logical way to wrestle control back, to give the health service and the government time to find a strategy to fight this. Treatments were still so new no one knew for sure what was best. It was a mess 5 - So we all suffered while doctors, sage and government learnt on the hoof, but as we gathered more and more information we began to understand the virus more, who it targeted, what treatments seemed to work, the fog of covid was lifting. 6 - Since March we have seen 43,000 lives lost. But we since March we have not moved from the panic days to what is now a very clear picture of the virus. Instead fear is still being used to cover up for the failures of the first half of the year. Fear is now the driver. 7 - But now we know much more, but we had a government desperate to stop the hemorrhaging of money and the suspension of the economy which to anyone with a brain can see will cost more lives over time. The country was facing a mental health breakdown. 8 - But we now know more, we have data from over a million days in hospitals, data from 43,000 deaths and the victims profiles, we have data on 450,000 survivors. We have insight on how big wave one was, the speed of spread and where. We have knowledge on treatments that work. 9 - we now know that the average age of victims is 82. We know which conditions make people vulnerable. We know the virus has a long incubation period and can stay on some surfaces for days. We know 10 times what we knew before lockdown. We know young people are resilient to it. 10 - What we also know is that by the time we see a covid spike any action we take is about mitigation. The spike is an indicator. We also know lockdowns are the worst form of treatment, the hammer and the flea. It is a steam roller to crack a nut. Lockdowns are madness. 11 - So what is the answer? Well we know many things, we know that by the time of national lockdown the cases were running at up to 120,000 new infections per day. Therefore it is highly likely we have seen since January 5 million people at least be infected. That is huge. 12 - We know that the virus mortality rate is not 3 or 4% its more like 0.13%. Statistically a person living in the UK has a 0.06% chance of dying of covid in their lifetime. Car accidents, robbings you are 6 times greater chance of being involved in. So why the fear, the panic? 13 - so now we know who it targets, who are resistant, how it spreads, who is at risk, the mortality of it, better treatments. The why are we still using the hammer? Fear. Fear of being wrong, fear of being blamed, fear of well just about anything covid related. 14 - So what is happening. #sage are badly advising thw government, they are not united as they seem. Too many egos, and quite frankly to many personal scientific theories to be proved before they see the damage they are doing to you, me the economy and our mental well-being. 15 - The data makes it very clear that a slightly more risky but far more sensible policy is controlled spread with shielding of our vulnerable people is a far better and sane, yes sane way forward. Why is it risky, well peoples lives of course. But that is why we must change. 16 - Lives, livelihoods, businesses, debt nental health, cancer patients, heart and lung patients all are dying either actually or financially or mentally now. Children at risk, educations destroyed. Because of Fear! 17 - The impact of this is going to ripple for a decade or more, lockdowns will kill far more than they save and that is a fact. Why? Because unlike #sage i see the impact of financial hardship on ordinary people, the damage is like a car crash, its long term effect deep! 18 - I have modelled recessions and impact for over twenty years, this gives me a terrible foresight on the damage we are now doing. I see the faces of those that will soon be flooding jobcentres desperate, in real crisis. Lives will be lost. Abuse will rise, crimes go up. 19 - I also see the damage we are doing to the nations health, trust me, the data is clear we are stocking up a health crisis so large it will hit every single family in every home everywhere, no one will escape untouched. And it is growing daily. It is insane self harm. 20 - 2 million cancer screenings lost. 62,000 urgent cancer referrals lost. A&amp;E referrals down 30%. Operations cancelled for up to 2 years. 1.5 million jobs lost. That will rise to 3 million over next 3 months. Businesses failing left right and centre. The tsunami is coming. 21 and last. I know a different way may seem scary, so much fear peddled by media and MPs like \"let it rip\" an awful nasty and evil set of words. But there is a better way and we all need to push for it. Be safe all. Statistics Guy\"\"\"\n",
    "%timeit preprocess_text(quite_long_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llegados a este punto, la última pregunta que nos podemos hacer es: ¿Realmente necesitamos un modelo tan potente como el de `Spacy`? ¿O si combinamos la rápida y bastante eficaz función `sent_tokenize`de la librería `nltk` con nuestra función será suficiente?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos una última prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Únicamente sustituiremos la línea:\n",
    "\n",
    "`sentences = [str(sent).strip() for sent in nlp(sentences).sents] # Spacy model`\n",
    "\n",
    "por\n",
    "\n",
    "`sentences = sent_tokenize(sentences)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "def preprocess_text(text, tokenizer=None, return_as_list=False):\n",
    "    if tokenizer is None:\n",
    "        # if next letter after period is lowercase, consider it part of the same sentence\n",
    "        # ex: \"As we can see in Figure 1.1. the sentence will not be split.\"\n",
    "        tokenizer = RegexpTokenizer(r'[^.!?]+[.!?]+[^A-Z]*')\n",
    "        # if there's no final period, add it (this makes the assumption that the last\n",
    "        # sentence is not interrogative or exclamative, i.e., ends with '?' or '!')\n",
    "        if text[-1] != '.' and text[-1] != '?' and text[-1] != '!':\n",
    "            text += '.'\n",
    "    \n",
    "    text = ' '.join(text.split()) # remove '\\n', '\\t', etc.\n",
    "    \n",
    "    sentences = ' '.join(tokenizer.tokenize(text)).replace('  ', ' ') # ensure there's 1 whitespace at most\n",
    "\n",
    "    sentences = sent_tokenize(sentences)\n",
    "\n",
    "    final_sentences = [sentences[0]]\n",
    "    \n",
    "    for sent in sentences[1:]:\n",
    "        # if the previous sentence doesn't end with a '.', '!' or '?' we concatenate the current sentence to it\n",
    "        if final_sentences[-1][-1] != '.' and final_sentences[-1][-1] != '!' and final_sentences[-1][-1] != '?':\n",
    "            final_sentences[-1] += ' ' + sent\n",
    "        # if the next sentence doesn't start with a letter or a number, concatenate it to the previous\n",
    "        elif not sent[0].isalpha() and not sent[0].isdigit():\n",
    "            final_sentences[-1] += sent\n",
    "        else:\n",
    "            final_sentences.append(sent)\n",
    "                                       \n",
    "    return final_sentences if return_as_list else ' '.join(final_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"How's your day going???!\", \"It's going... Let's just say it's not going to bad.\"]\n",
      "['Hello.', 'Goodbye.']\n",
      "['Seriously??!', \"That can't be true.\"]\n",
      "['Mr. Elster looked worried.']\n",
      "['London is the capital of U. K.']\n",
      "['I was born in 02.28.1980 in New York.']\n",
      "['She asked \"How\\'s it going?\", and I said \"Great!\".']\n",
      "['As we can see in Figure 1.1. the model will fail.']\n"
     ]
    }
   ],
   "source": [
    "examples = [\"How's your        day going???!It's     going...\\n Let's just say it's not going to \\t bad.\",\n",
    "            \"Hello.Goodbye.\",\n",
    "            \"Seriously??!That can't be true.\",\n",
    "            \"Mr. Elster looked worried.\",\n",
    "            \"London is the capital of U.K.\",\n",
    "            \n",
    "            \"I was born in 02.28.1980 in New York\",\n",
    "            \"She asked \\\"How's it going?\\\", and I said \\\"Great!\\\"\",\n",
    "            \"As we can see in Figure 1.1. the model will fail.\"]\n",
    "\n",
    "for text in examples:\n",
    "    print(preprocess_text(text, return_as_list=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la única diferencia es que en el primer ejemplo ha tomado las frases separadas con puntos suspensivos como una sola frase, lo cual no es muy grave ya que en muchas ocasiones efectivamente se trata de una única frase (p. ej.: \"Digamos que estuvo... interesante\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, los dos últimos ejemplos lo ha clasificado correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Midamos el tiempo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7 ms ± 7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "quite_long_text = \"\"\"Our country is being held hostage by mad scientists and MPs afraid of blame.And the most important vision they should be using, hindsight they all appear to be blind too. I will try and break it down in this long thread. #WhyAreTheyDoingThis #pensionerprisoners #Covid_19 1 - Covid-19 arrived far earlier than anyone knew. It was likely in Britain in November or December and by January and February it was spreading rapidly. This took our government and other countries by surprise. And the resulting overload on health services was massive. 2 - there was little choice by mid March for most leaders and action to prevent a catastrophic failure and collapse of the NHS was essential. Cases were flooding in stadd sickness in NHS was rising fast. We were on the brink. 3 - the virus was so new the medical knowledge just hadn\\'t accumulated to make good decisions on treatments. Doctors were fighting for peoples lives based on at times best guess using all their previous knowledge of other viruses. But covid was different and its impact nuanced. 4 - Although lockdown was a hammer to hit a flea it seemed at the time the only logical way to wrestle control back, to give the health service and the government time to find a strategy to fight this. Treatments were still so new no one knew for sure what was best. It was a mess 5 - So we all suffered while doctors, sage and government learnt on the hoof, but as we gathered more and more information we began to understand the virus more, who it targeted, what treatments seemed to work, the fog of covid was lifting. 6 - Since March we have seen 43,000 lives lost. But we since March we have not moved from the panic days to what is now a very clear picture of the virus. Instead fear is still being used to cover up for the failures of the first half of the year. Fear is now the driver. 7 - But now we know much more, but we had a government desperate to stop the hemorrhaging of money and the suspension of the economy which to anyone with a brain can see will cost more lives over time. The country was facing a mental health breakdown. 8 - But we now know more, we have data from over a million days in hospitals, data from 43,000 deaths and the victims profiles, we have data on 450,000 survivors. We have insight on how big wave one was, the speed of spread and where. We have knowledge on treatments that work. 9 - we now know that the average age of victims is 82. We know which conditions make people vulnerable. We know the virus has a long incubation period and can stay on some surfaces for days. We know 10 times what we knew before lockdown. We know young people are resilient to it. 10 - What we also know is that by the time we see a covid spike any action we take is about mitigation. The spike is an indicator. We also know lockdowns are the worst form of treatment, the hammer and the flea. It is a steam roller to crack a nut. Lockdowns are madness. 11 - So what is the answer? Well we know many things, we know that by the time of national lockdown the cases were running at up to 120,000 new infections per day. Therefore it is highly likely we have seen since January 5 million people at least be infected. That is huge. 12 - We know that the virus mortality rate is not 3 or 4% its more like 0.13%. Statistically a person living in the UK has a 0.06% chance of dying of covid in their lifetime. Car accidents, robbings you are 6 times greater chance of being involved in. So why the fear, the panic? 13 - so now we know who it targets, who are resistant, how it spreads, who is at risk, the mortality of it, better treatments. The why are we still using the hammer? Fear. Fear of being wrong, fear of being blamed, fear of well just about anything covid related. 14 - So what is happening. #sage are badly advising thw government, they are not united as they seem. Too many egos, and quite frankly to many personal scientific theories to be proved before they see the damage they are doing to you, me the economy and our mental well-being. 15 - The data makes it very clear that a slightly more risky but far more sensible policy is controlled spread with shielding of our vulnerable people is a far better and sane, yes sane way forward. Why is it risky, well peoples lives of course. But that is why we must change. 16 - Lives, livelihoods, businesses, debt nental health, cancer patients, heart and lung patients all are dying either actually or financially or mentally now. Children at risk, educations destroyed. Because of Fear! 17 - The impact of this is going to ripple for a decade or more, lockdowns will kill far more than they save and that is a fact. Why? Because unlike #sage i see the impact of financial hardship on ordinary people, the damage is like a car crash, its long term effect deep! 18 - I have modelled recessions and impact for over twenty years, this gives me a terrible foresight on the damage we are now doing. I see the faces of those that will soon be flooding jobcentres desperate, in real crisis. Lives will be lost. Abuse will rise, crimes go up. 19 - I also see the damage we are doing to the nations health, trust me, the data is clear we are stocking up a health crisis so large it will hit every single family in every home everywhere, no one will escape untouched. And it is growing daily. It is insane self harm. 20 - 2 million cancer screenings lost. 62,000 urgent cancer referrals lost. A&amp;E referrals down 30%. Operations cancelled for up to 2 years. 1.5 million jobs lost. That will rise to 3 million over next 3 months. Businesses failing left right and centre. The tsunami is coming. 21 and last. I know a different way may seem scary, so much fear peddled by media and MPs like \"let it rip\" an awful nasty and evil set of words. But there is a better way and we all need to push for it. Be safe all. Statistics Guy\"\"\"\n",
    "%timeit preprocess_text(quite_long_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es unas 60-70 veces más rápido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta última versión sigue sin ser perfecta. Por ejemplo, fallaría en el siguiente caso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP (i.e.',\n",
       " 'Natural Language Processing) is a subfield of Linguistics, Computer Science, and Artificial Intelligence.']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(\"NLP (i.e. Natural Language Processing) is a subfield of Linguistics, \" +\n",
    "                \"Computer Science, and Artificial Intelligence.\", return_as_list=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que divide la frase en dos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No obstante, si añadiésemos una coma tras el i.e. (como suele ser común), sí funcionaría correctamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tomorrow I work the morning shift, i.e., from 6 am to 1 pm.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(\"Tomorrow I work the morning shift, i.e., from 6 am to 1 pm.\", return_as_list=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como conclusión, cabe admitir que es muy complicado desarrollar un modelo que se ajuste a todos y cada uno de los casos.\n",
    "\n",
    "Recordemos que lo que buscábamos era:\n",
    "- \"Limpiar\" y formatear el texto correctamente.\n",
    "- Identificar las frases en el texto. Esto nos permite dividirlo con el fin de ajustarlo a la entrada máxima de los modelos de resumen.\n",
    "\n",
    "En nuestro caso, creemos haber encontrado un buen compromiso entre *precisión* y *eficiencia*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actualización (v0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiempo después de escribir este notebook, dimos con otra librería, esta vez de Microsoft, que también implementa una función de división de frases (`text_to_sentence`). Tras varias pruebas rápidas, se comprobó que, a primera vista, trabaja mejor que la función `sent_tokenize` de NLTK.\n",
    "\n",
    "Modifiquemos la función que implementamos anteriormente para comprobar si esto es cierto.\n",
    "\n",
    "Además, también modificaremos la expresión regular utilizada para el `tokenizer`, de forma que capture siglas que contengan puntos, por ejemplo: `U.K.`, `U.S`, `A.K.A.`, `R.I.P.`, etc. Este tipo de cadenas las capturaremos con la siguiente expresión: `(?:[A-Z][.])+`, es decir, coge una letra y un punto, una o más veces. El `?:` indica un grupo sin captura, a fin de que coincida con la expresión completa, y no solo con el último grupo.\n",
    "\n",
    "Con esto resolvemos el problema que existía anteriormente que provocaba que se insertara un espacio en las siglas que contenían puntos, es decir, `U. K.`, en vez de `U.K.`, `B. C.`, en vez de `B.C.`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from blingfire import text_to_sentences\n",
    "\n",
    "def preprocess_text(text, tokenizer=None, return_as_list=False):\n",
    "    if tokenizer is None:\n",
    "        # if next letter after period is lowercase, consider it part of the same sentence\n",
    "        # ex: \"As we can see in Figure 1.1. the sentence will not be split.\"\n",
    "        tokenizer = RegexpTokenizer(r'[^.!?]+(?:(?:[A-Z][.])+|[.!?]+)+[^A-Z]*')\n",
    "        # if there's no final period, add it (this makes the assumption that the last\n",
    "        # sentence is not interrogative or exclamative, i.e., ends with '?' or '!')\n",
    "        if text[-1] != '.' and text[-1] != '?' and text[-1] != '!':\n",
    "            text += '.'\n",
    "    \n",
    "    text = ' '.join(text.split()) # remove '\\n', '\\t', etc.\n",
    "    \n",
    "    sentences = ' '.join(tokenizer.tokenize(text)).replace('  ', ' ') # ensure there's 1 whitespace at most\n",
    "\n",
    "    sentences = text_to_sentences(sentences).split('\\n')\n",
    "\n",
    "    final_sentences = [sentences[0]]\n",
    "    \n",
    "    for sent in sentences[1:]:\n",
    "        # if the previous sentence doesn't end with a '.', '!' or '?' we concatenate the current sentence to it\n",
    "        if final_sentences[-1][-1] != '.' and final_sentences[-1][-1] != '!' and final_sentences[-1][-1] != '?':\n",
    "            final_sentences[-1] += ' ' + sent\n",
    "        # if the next sentence doesn't start with a letter or a number, concatenate it to the previous\n",
    "        elif not sent[0].isalpha() and not sent[0].isdigit():\n",
    "            final_sentences[-1] += sent\n",
    "        else:\n",
    "            final_sentences.append(sent)\n",
    "                                       \n",
    "    return final_sentences if return_as_list else ' '.join(final_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"How's your day going???!\", \"It's going...\", \"Let's just say it's not going to bad.\"]\n",
      "['Hello.', 'Goodbye.']\n",
      "['Seriously??!', \"That can't be true.\"]\n",
      "['Mr. Elster looked worried.']\n",
      "['London is the capital of U.K.']\n",
      "['I was born in 02.28.1980 in New York.']\n",
      "['She asked \"How\\'s it going?\", and I said \"Great!\".']\n",
      "['As we can see in Figure 1.1. the model will fail.']\n"
     ]
    }
   ],
   "source": [
    "examples = [\"How's your        day going???!It's     going...\\n Let's just say it's not going to \\t bad.\",\n",
    "            \"Hello.Goodbye.\",\n",
    "            \"Seriously??!That can't be true.\",\n",
    "            \"Mr. Elster looked worried.\",\n",
    "            \"London is the capital of U.K.\",\n",
    "            \"I was born in 02.28.1980 in New York\",\n",
    "            \"She asked \\\"How's it going?\\\", and I said \\\"Great!\\\"\",\n",
    "            \"As we can see in Figure 1.1. the model will fail.\"]\n",
    "\n",
    "for text in examples:\n",
    "    print(preprocess_text(text, return_as_list=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que todo sigue funcionando correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos con el caso en el que antes fallaba nuestra función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP (i.e. Natural Language Processing) is a subfield of Linguistics,Computer Science, and Artificial Intelligence.']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(\"NLP (i.e. Natural Language Processing) is a subfield of Linguistics, \" +\n",
    "                \"Computer Science, and Artificial Intelligence.\", return_as_list=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Funciona!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

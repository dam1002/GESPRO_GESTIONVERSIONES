{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/dmlls/jizt/c2d7b9b81783e298d1898b5743b147d1faff8f29/images/JIZT-logo.svg\" title=\"JIZT\" alt=\"JIZT\" width=\"230\" align=\"left\" style=\"margin-top:15px;margin-right:30px;\" />\n",
    "\n",
    "---\n",
    "\n",
    "### Post-procesamiento básico del texto\n",
    "[Diego Miguel Lozano](https://github.com/dmlls) \\\n",
    "GPL-3.0 License\n",
    "\n",
    "*Última actualización: 9 de noviembre de 2020*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "Este notebook se centra en el proceso de post-procesado del texto de salida de los modelos `BART`y `T5`, una vez llevado a cabo el proceso de resumen. Los textos empleados como ejemplo están en inglés, dado que estos modelos están optimizados para este idioma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requerimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder ejecutar este notebook, se debe tener instalada la última versión de los siguientes paquetes:\n",
    "\n",
    "- `NLTK`\n",
    "- `SpaCy` con `en_core_web_sm`\n",
    "- [`truecase`](https://github.com/daltonfury42/truecase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-procesamiento del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El post-procesamiento del texto que vamos a llevar a cabo va a consistir en:\n",
    "- Formatear correctamente el texto. Esta tarea consiste fundamentalmente en eliminar espacios innecesarios (o añadirlos, p. ej., al inicio de una frase intermedia).\n",
    "- Añadir mayúsculas allá donde sea necesario, principalmente:\n",
    "  - Al comienzo de cada frase.\n",
    "  - En el caso de nombres de personas, países, organizaciones, etc. Dicho de forma más precisa, en el caso de aquellas entidades reconocidas que requieran mayúsculas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que haremos será dividir el texto en frases para poder añadir un espacio al inicio de cada frase intermedia.\n",
    "\n",
    "Para ello, nos vamos a ayudar de la función que desarrollamos en el notebook [Pre-procesamiento del texto](https://github.com/dmlls/jizt/blob/main/notebooks/Preprocesamiento%20del%20texto.ipynb). Allí se encuentra una explicación detallada de cómo se llegó a ella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import sent_tokenize\n",
    "from typing import List, Optional\n",
    "\n",
    "def sentence_tokenize(text: str, tokenizer: Optional[RegexpTokenizer] = None) -> List[str]:\n",
    "    \"\"\"Divides the text into sentences.\n",
    "    The steps followed are:\n",
    "        - Remove characters such as '\\n', '\\t', etc.\n",
    "        - Splits the text into sentences, taking into account Named Entities and\n",
    "        special cases such as:\n",
    "        + \"I was born in 02.26.1980 in New York\", \"As we can see in Figure 1.1.\n",
    "            the model will not fail.\": despite the periods in the date and the\n",
    "            Figure number, these texts will not be split into different sentences.\n",
    "        + \"Mr. Elster looked worried.\", \"London, capital of U.K., is famous\n",
    "            for its red telephone boxes\": the preprocessor applies Named Entity\n",
    "            Recognition and does not split the previous sentences.\n",
    "        + \"Hello.Goodbye.\", \"Seriously??!That can't be true.\": these sentences\n",
    "            are split into: ['Hello.', 'Goodbye.'] and ['Seriously??!', 'That can't\n",
    "            be true.'], respectively.\n",
    "    \n",
    "    Args:\n",
    "        text:\n",
    "            Text to be split in sentences.\n",
    "        tokenizer:\n",
    "            Regular expression to carry out a preliminar split (the text will be\n",
    "            afterwards split once again by the NLTK `sent_tokenize` method).\n",
    "    \"\"\"\n",
    "\n",
    "    # punctuation that shouldn't be preceeded by a whitespace\n",
    "    PUNCT_NO_PREV_WHITESPACE = \".,;:!?\"\n",
    "\n",
    "    if tokenizer is None:\n",
    "        # if next letter after period is lowercase, consider it part of the same sentence\n",
    "        # ex: \"As we can see in Figure 1.1. the sentence will not be split.\"\n",
    "        tokenizer = RegexpTokenizer(r'[^.!?]+[.!?]+[^A-Z]*')\n",
    "\n",
    "    # if there's no final period, add it (this makes the assumption that the last\n",
    "    # sentence is not interrogative or exclamative, i.e., ends with '?' or '!')\n",
    "    if text[-1] != '.' and text[-1] != '?' and text[-1] != '!':\n",
    "        text += '.'\n",
    "\n",
    "    text = ' '.join(text.split()) # remove '\\n', '\\t', etc.\n",
    "    \n",
    "    # split sentences with the regexp and ensure there's 1 whitespace at most\n",
    "    sentences = ' '.join(tokenizer.tokenize(text))\n",
    "    \n",
    "    # remove whitespaces before PUNCT_WITHOUT_PREV_WHITESPACE\n",
    "    for punct in PUNCT_NO_PREV_WHITESPACE:\n",
    "        sentences = sentences.replace(' ' + punct, punct)\n",
    "\n",
    "    sentences = sent_tokenize(sentences)\n",
    "\n",
    "    final_sentences = [sentences[0]]\n",
    "\n",
    "    for sent in sentences[1:]:\n",
    "        # if the previous sentence doesn't end with a '.', '!' or '?'\n",
    "        # we concatenate the current sentence to it\n",
    "        if final_sentences[-1][-1] != '.' and \\\n",
    "            final_sentences[-1][-1] != '!' and \\\n",
    "            final_sentences[-1][-1] != '?':\n",
    "            final_sentences[-1] += (' ' + sent)\n",
    "        # if the next sentence doesn't start with a letter or a number,\n",
    "        # we concatenate it to the previous\n",
    "        elif not sent[0].isalpha() and not sent[0].isdigit():\n",
    "            final_sentences[-1] += sent\n",
    "        else:\n",
    "            final_sentences.append(sent)\n",
    "    \n",
    "    return final_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ventaja de esta función, aparte de identificar las frases en un texto, es que también nos elimina elementos como espacios o tabuladores innecesarios, lo cual nos interesa para el correcto formateo del texto final.\n",
    "\n",
    "Como primer paso para formatear correctamente un texto, uniremos las frases que nos devuleve el anterior método, dejando un espacio entre cada una de ellas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a sentence. Ups! Someone forgot to add spaces after each sentence.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a sentence.Ups!Someone forgot to add spaces after each sentence.\"\n",
    "\n",
    "' '.join(sentence_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, llevaremos a cabo un proceso de Reconocimiento de Entidades Nombradas (NER, por sus siglas en inglés), con el fin de añadir mayúsculas a aquellas entidades que lo requieran.\n",
    "\n",
    "A la hora de llevar a cabo NER, dos de las opciones más ampliamente utilizadas se basan en el uso de las librerías [NLTK](https://www.nltk.org/), [SpaCy](https://spacy.io/) o [CoreNLP](https://stanfordnlp.github.io/CoreNLP/). Si atendemos al [estudio comparativo realizado por Analytics Vidhya](https://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%E2%80%8Bin-python/), la librería que mejores resultados ofrece en NER es CoreNLP. No obstante, también es la más lenta.\n",
    "\n",
    "En nuestra opinión, parece que la librería SpaCy ofrece un buen compromiso entre precisión y velocidad: es más precisa que NLTK (aunque más lenta), y más rápida que CoreNLP (aunque menos precisa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Donald Trump ........................... PERSON\n",
      " Twitter ................................ ORG\n",
      " Monday ................................. DATE\n",
      " Defense Mark Esper ..................... ORG\n",
      " Christopher Miller ..................... PERSON\n",
      " the National Counterterrorism Center ... ORG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"\"\"President Donald Trump announced on Twitter Monday that he has fired Secretary\n",
    "          of Defense Mark Esper, and that Christopher Miller, who serves as director of\n",
    "          the National Counterterrorism Center, will become acting secretary \"effective\n",
    "          immediately.\"\"\" # source: https://lite.cnn.com/en/article/h_9a38ab64c73389fb61475e431c0893e3\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\" {ent.text + ' ':.<40} {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo parece estar trabajando perfectamente en el caso anterior. Ha reconocido todas las entidades nombradas. Sin embargo, esto cambia bastante cuando pasamos todo el texto a minúsculas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " donald trump ........................... PERSON\n",
      " monday ................................. DATE\n",
      " christopher miller ..................... PERSON\n"
     ]
    }
   ],
   "source": [
    "text = text.lower() # pasar el texto a minúsculas\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\" {ent.text + ' ':.<40} {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, solo ha identificado correctamente un 50% de las entidades nombradas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echemos un vistazo a cómo la librería NLTK se comporta en este caso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Donald Trump ........................... PERSON\n",
      " Twitter ................................ PERSON\n",
      " Defense Mark Esper ..................... ORGANIZATION\n",
      " Christopher Miller ..................... PERSON\n",
      " National Counterterrorism Center ....... ORGANIZATION\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ne_chunk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "text = \"\"\"President Donald Trump announced on Twitter Monday that he has fired Secretary\n",
    "          of Defense Mark Esper, and that Christopher Miller, who serves as director of\n",
    "          the National Counterterrorism Center, will become acting secretary \"effective\n",
    "          immediately.\"\"\" # source: https://lite.cnn.com/en/article/h_9a38ab64c73389fb61475e431c0893e3\n",
    "\n",
    "ne_chunks = nltk.ne_chunk(nltk.pos_tag(word_tokenize(text)))\n",
    "\n",
    "# print with a nice format\n",
    "for chunk in ne_chunks:\n",
    "    if type(chunk) == Tree: # get only NEs\n",
    "        ent_text = \" \".join([token for token, _ in chunk.leaves()])\n",
    "        ent_label = chunk.label()\n",
    "        print(f\" {ent_text + ' ':.<40} {ent_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que ha identificado correctamente todas las entidades nombradas (NLTK, a diferencia de SpaCy, no incluye las fechas como entidades nombradas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero, de nuevo, el problema persiste. Al eliminar las mayúsculas de las entidades nombradas, NLTK no es capaz de reconocer ninguna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower() # pasar el texto a minúsculas\n",
    "\n",
    "ne_chunks = nltk.ne_chunk(nltk.pos_tag(word_tokenize(text)))\n",
    "\n",
    "# print with a nice format\n",
    "for chunk in ne_chunks:\n",
    "    if type(chunk) == Tree: # get only NEs\n",
    "        ent_text = \" \".join([token for token, _ in chunk.leaves()])\n",
    "        ent_label = chunk.label()\n",
    "        print(f\" {ent_text + ' ':.<40} {ent_label}\") # nothing to print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra posibilidad podría ser poner en mayúsculas la primera letra de cada palabra y posteriormente realizar el NER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Donald Trump ........................... PERSON\n",
      " On Twitter ............................. PERSON\n",
      " Monday ................................. DATE\n",
      " Christopher Miller ..................... PERSON\n",
      " The National Counterterrorism Center ... ORG\n",
      " Will Become ............................ PERSON\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"President Donald Trump announced on Twitter Monday that he has fired Secretary\n",
    "          of Defense Mark Esper, and that Christopher Miller, who serves as director of\n",
    "          the National Counterterrorism Center, will become acting secretary \"effective\n",
    "          immediately.\"\"\" # source: https://lite.cnn.com/en/article/h_9a38ab64c73389fb61475e431c0893e3\n",
    "\n",
    "text = text.lower()\n",
    "\n",
    "text = \" \".join(word.capitalize() for word in text.split())\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\" {ent.text + ' ':.<40} {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto ha mejorado la predicción, pero también ha introducido valores incorrectos (i.e., `On Twitter`, `Will Become`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulta que nos encontramos ante un problema mucho más complejo de lo que parecía en un primer momento. Al proceso de corregir el uso de mayúsculas en un texto se lo conoce como \"truecasing\". Existe un [conocido artículo](https://www.cs.cmu.edu/~llita/papers/lita.truecasing-acl2003.pdf) ofreciendo una posible solución estadística al problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras la publicación de este artículo, aparecieron varias implementaciones del mismo en Python. Una de esas implementaciones nos la proporciona la librería [`truecase`](https://github.com/daltonfury42/truecase):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'President Donald Trump announced on Twitter Monday that he has fired Secretary of defense mark Esper, and that Christopher Miller, who serves as director of the National Counterterrorism center, will become acting Secretary\" effective immediately.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from truecase import get_true_case\n",
    "\n",
    "text = \"\"\"President Donald Trump announced on Twitter Monday that he has fired Secretary\n",
    "          of Defense Mark Esper, and that Christopher Miller, who serves as director of\n",
    "          the National Counterterrorism Center, will become acting secretary \"effective\n",
    "          immediately.\"\"\" # source: https://lite.cnn.com/en/article/h_9a38ab64c73389fb61475e431c0893e3\n",
    "\n",
    "text = text.lower() # not necessary, truecase already does this\n",
    "\n",
    "get_true_case(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado no es perfecto (i.e. `Secretary of defense mark Esper`, `National Counterterrorism center`), pero es el mejor obtenido hasta ahora. Además, el modelo que emplea por defecto `truecase` está entrenado sobre el corpus de inglés de NLTK. Entrenar un nuevo modelo es relativamente sencillo, así que en un futuro se podría probar a entrenar el modelo con un corpus más grande (p. ej., un volcado reciente de Wikipedia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniendo todo junto, ya tenemos una primera y sencilla versión de nuestro post-procesador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from truecase import get_true_case\n",
    "\n",
    "def post_process(text):\n",
    "    txt = ' '.join(sentence_tokenize(text))\n",
    "    return get_true_case(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covid is a pandemic that's been rife in the U. S. for the past 9 months. It's not affected their daily life any more than fewer tourists. It's as if I have arrived with my mask from a foreign planet and they were all curious to hear what my hell has looked like for the recent 9 months. \n",
      "\n",
      "Neko Wilson was incarcerated on July 2, 2019 because of a probation violation for a 17 year old marijuana case. The U. S. criminal legal system is death. In 2003, Neko was searched for being a black passenger in a speeding car. \n",
      "\n",
      "The history of mutual funds in India can be broadly divided into four distinct phases: first phase( 1964-1987) in 1963, government of India and Reserve Bank of India came together to form Uti( unit trust of India) and it is the first mutual fund in India. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = [\"covid is a pandemic that's been rife in the u.s. for the past 9 \" +\n",
    "            \"months. it's not affected their daily life any more than fewer \" +\n",
    "            \"tourists.It's as if i have arrived with my mask from a foreign planet \" +\n",
    "            \"and they were all curious to hear what my hell has looked like \" +\n",
    "            \"for the recent 9 months\",\n",
    "         \"neko wilson was incarcerated on july 2, 2019 because of a \" +\n",
    "            \"probation violation for a 17 year old marijuana case. the u.s. \" +\n",
    "            \"criminal legal system is death. in 2003, neko was searched for \" +\n",
    "            \"being a black passenger in a speeding car.\",\n",
    "         \"the history of mutual funds in india can be broadly divided into \" +\n",
    "            \"four distinct phases: first phase (1964-1987) in 1963, \" +\n",
    "            \"government of india and reserve bank of india came together \" +\n",
    "            \"to form uti (unit trust of india) and it is the first \"\n",
    "            \"mutual fund in india.\"]\n",
    "\n",
    "for txt in texts:\n",
    "    print(post_process(txt), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que `truecase` está introduciendo pequeños errores de espacios (`U. S`, `It' s`, `Uit( unit [...])`). Esto se debe a que la forma en la que `truecase` junta los tókenes una vez corregidas las mayúsculas es demasiado simple, y no contempla casos más especiales.\n",
    "\n",
    "En una versión posterior del Post-procesador, se modificará el código de `truecase` para resolver estos pequeños problemas.\n",
    "\n",
    "Por ahora, podemos dar nuestra versión 0.1 del Post-procesador como finalizada."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
